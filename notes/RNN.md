# 循环神经网络

## 什么是循环神经网络

## 序列

普通神经网络或者卷积神经网络都有一个局限性，就是他们的API限制太强：输入需要是一个大小经过修正的向量，比如一个图片，产生的也是一个大小经过修正的向量，比如不同分类的概率，并且这些模型使用固定的计算步骤来计算这种映射，比如模型的层数。而循环神经网络一个核心的优势在于允许对一个序列的向量进行操作，可以序列作为输入，作为输出，或者两者皆是序列，下图给了一些示例：

![1573567513827](D:\ML Projects\Pytorch-DL\notes\img\1573567513827.png)

图中每个方块代表一个向量，箭头代表函数（例如矩阵乘法），输入向量是红色，输出向量是蓝色，绿色代表RNN的状态向量（稍后会介绍）

那么从左到右依次是：

1. 输入向量，输出向量，普通神经网络或者卷积神经网络，从一个修正后的输入到一个修正后的输出，例如图片分类，输入是把图片像素修正后的向量，输出是各个分类的概率组成的向量
2. 输入向量，输出序列，例如看图说话，给一张图片，输出一句描述的话
3. 输入序列，输出向量，例如情感分析，给一个句子，输出是正面还是负面
4. 输入序列，输出序列，例如机器翻译
5. 同步序列输入和输出，例如给视频的每一帧分类

可以看到与从一开始就要经过固定数量的计算步骤的网络相比，序列操作机制要强大的多，RNNs通过把输入向量和状态向量结合起来得到新的状态向量

## RNN 计算过程：

RNNs可以看做一个很简单的API：接收一个输入向量x，返回一个输出向量y，然而输出向量不仅仅受输入的影响，还收到整个输入历史的影响

宏观来看RNN就是：

```python
rnn = RNN()
y = rnn.step(x)
```

仔细看RNN的内部其实有一些隐藏状态，每次调用step都会更新，下面是一个step函数的简单代码展示：

```python
class RNN:
	def step(self, x):
		# update the hidden state
		self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
		# compute the output vector
		y = np.dot(self.W_hy, self.h)
		return y
```

上面是一个RNN的简单的前向传播计算，RNN有三个权重参数（这里我们忽略偏置）W_hh, W_xh, W_hy，隐藏状态self.h初始化是0向量，np.tahn是非线性激活函数，把值域压缩到[-1,1]，数学表示就是

$h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t)$

$y = W_{hy}h_t$

序列的第一个值x0传入时，和初始状态向量h0加权求和，通过激活函数，得到新的状态向量h1，保留下来，同时通过h1计算第一个输出y0

序列的第二个值x1传入时，和上一个时间步保留的状态向量h1加权求和再激活，得到新状态向量h2，保留下来，同时通过h2计算第二个输出y1

以此类推

## 字符级的语言模型

现在我们对RNN有了大致的认识，但是为什么RNN有用，它们是怎么运行的？

让我们来举一个例子，我们训练一个字符级别的RNN语言模型，给模型一段文本，让它在给定前一个

假设我们的词库里有四个字母“helo"，然后我们想在一个序列”hello“上训练一个RNN。训练序列其实可以分成四个训练样本：1. 在给定h的条件下，下一个字符是e的概率 2. 在给定he的条件下，下一个字符是l的概率 3. 在给定hel的条件下，下一个字符是l的概率 4. 再给定hell， 下一个字符是o的概率，

过程如下图：

![1573573389055](D:\ML Projects\Pytorch-DL\notes\img\1573573389055.png)

